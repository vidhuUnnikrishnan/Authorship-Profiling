{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship Profiling\n",
    
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "# import multiprocessing as mp\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from nltk.corpus import wordnet\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's parse the xml files given to us. For that we are creating a function so that all the files from a given path will be parsed and saved as separate tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseXML(xmlFile):\n",
    "    root = ET.parse(xmlFile).getroot()\n",
    "    tweets = []\n",
    "    for elem in root:\n",
    "        for subelem in elem:\n",
    "            tweets.append(subelem.text)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_xmls = [] #empty list to append the dictionaries with tweet id and tweets\n",
    "path = './data/xmls' #path with all the xml files\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    authorTweets = {}\n",
    "    if not filename.endswith('.xml'): continue #if file is not xml skip it\n",
    "    file = os.path.join(path, filename)\n",
    "    authorTweets['id'] = os.path.splitext(filename)[0] #remove file extension\n",
    "    authorTweets['tweets'] = parseXML(file)\n",
    "    parsed_xmls.append(authorTweets) #returns lsit appended with dictionaries with id and tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the parsed documents are in a list of dictionaries now. Let's preprocess the tweets and extract some features.\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "Here we are creating a function which takes the parsed xml as argument and returns a dictionary with tweet id, the original tweets, unigram tokens based on Bag-of-Words approach, unigram tokens based of Parts-Of-Speech approach (tokens and tags together), and the tags as features. They are returns as list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "def tokenizeRawData(parsedTweets):\n",
    "    token_tweets = []\n",
    "    alpha_tweets = []\n",
    "    tokenised_tweets = {}\n",
    "    for tweet in parsedTweets['tweets']:\n",
    "        word_tokens = nltk.tokenize.word_tokenize(tweet.lower())\n",
    "        token_tweets.extend(word_tokens)\n",
    "    for each in token_tweets:\n",
    "        if each.isalpha():\n",
    "            alpha_tweets.append(each)\n",
    "            \n",
    "    tokenised_tweets['id'] = parsedTweets['id']\n",
    "    tokenised_tweets['tweets'] = tweet\n",
    "    tokenised_tweets['bow_tokens'] = [wnl.lemmatize(each) for each in alpha_tweets]\n",
    "    tokenised_tweets['pos_tokens'] = nltk.pos_tag(alpha_tweets)\n",
    "    tokenised_tweets['pos_tags'] = [k for _,k in tokenised_tweets['pos_tokens']]\n",
    "    return(tokenised_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenized_tweets = [] \n",
    "for each in parsed_xmls:\n",
    "    tokenized_tweets.append(tokenizeRawData(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ease of viewing features and computational efficiency, let's change the list to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_df = pd.DataFrame(tokenized_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting to dataframe, we can easily remove the stopwords from the unigrams with the bag of words approach using lambda apply function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords removal from both bag of words and parts of speech\n",
    "stopwords = []\n",
    "with open('./data/stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "    \n",
    "tweet_df['bow_tokens'] = tweet_df['bow_tokens'].apply(lambda x: [item for item in x if item not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from itertools import chain\n",
    "word_list = tweet_df['bow_tokens']\n",
    "words = [item for sublist in word_list for item in sublist]\n",
    "vocab = set(words)\n",
    "lexical_diversity = len(words)/len(vocab)\n",
    "print (\"Vocabulary size: \",len(vocab),\"\\nTotal number of tokens: \", len(words), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we progress, let's see if there is any words that are appearing across all the documents. Those ones add very little value to our prediction tasks. We use FreqDist function to count the frequency of these words. We are not counting how many times a word is being used by an author, but which words are being used by almost all the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import *\n",
    "words_2 = list(chain.from_iterable([set(value) for value in word_list]))\n",
    "fd_2 = FreqDist(words_2)\n",
    "fd_2.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like \"https\" is used by more than 98% of the authors. This might be the remaining of any url links which was removed in our preporcessing step. So we can definitely remove that. On an assumption that, the frequency of words follow normal distribution (which will be the case if our dataset increase by CLT), lets keep all the words used by less than 95% of the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['bow_tokens'] = tweet_df['bow_tokens'].apply(lambda x: [item for item in x if item != 'https'])\n",
    "\n",
    "#new column with the count of all tokens after stopwords and most common words\n",
    "tweet_df['token_count'] = tweet_df['bow_tokens'].apply(lambda words: len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are calculating the sentiment composite score to see if there is any significance difference in tweets by male and female authors. For that we are using SentimentIntensityAnalyzer function. We can visualise the affect later when we split the data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "tweet_df['sentiment'] = tweet_df['tweets'].apply(lambda t: sid.polarity_scores(t)['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are joining all the tokens into sentences. We are creating three string features. \n",
    "1. Token string: String of all unigrams under Bag-of-Words approach\n",
    "2. POS String: String of all tags under Parts-Of-Speech\n",
    "3. Combined token and pos: Combined string of both Bag-of-Words tokens and Parts-Of-Speech tags.\n",
    "\n",
    "We are going to check for which of these we get a better classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['token_string'] = tweet_df['bow_tokens'].apply(lambda word: ' '.join(word))\n",
    "tweet_df['pos_string'] = tweet_df['pos_tags'].apply(lambda word: ' '.join(word))\n",
    "tweet_df['token_pos_string'] = tweet_df[['token_string', 'pos_string']].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have preprocessed all of our tweets documents including both training examples and test data. Since, we are going to create models, we need to separate the training and test data as we need labels to train and test data doesn't have that. We are loading the csv files given to us to separate the training and testing using the ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_csv(\"./data/train_labels.csv\")\n",
    "testing_df = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "training_id = list(training_df['id'])\n",
    "testing_id = list(testing_df['id'])\n",
    "tweet_df_train = tweet_df[tweet_df['id'].isin(training_id)]\n",
    "tweet_df_test = tweet_df[tweet_df['id'].isin(testing_id)]\n",
    "\n",
    "tweet_df_train = tweet_df_train.merge(training_df[['id', 'gender']], on='id', how='left')\n",
    "tweet_df_train['gender'][tweet_df_train['gender'] == 'male'] = 1\n",
    "tweet_df_train['gender'][tweet_df_train['gender'] == 'female'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the labels for training examples, let's see if the sentiment score we calculated show any significance difference for male and female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "gender = [1, 0]\n",
    "for gender in gender:\n",
    "    sns.distplot(tweet_df_train[tweet_df_train['gender'] == gender]['sentiment'], label = gender, hist=False)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the tweets by both category of authors follow very similar distribution. So we are not going to use them as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "\n",
    "Let's first try with just Token string on all the seven models and save all the accuracy scores to a dataframe.\n",
    "\n",
    "The commemnted out sections are computing accuracy for different models on count vector, tf-idf, and ngrams of the three \"Token String\", \"POS String\" and \"Token POS String\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_token_df = pd.DataFrame(np.nan, index=[0, 1, 2], columns=['Features', 'NaiveBayes', 'LogisticRegression', 'SupportVectorMachine', \n",
    "#                                                                 'RandomForest', 'XGBoost', 'NeuralNet'])\n",
    "# accuracy_token_df['Features'] = ['CountVector', 'WordLevelTF_IDF', 'N-GramVector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "#     random.seed(3000)\n",
    "#     # fit the training dataset on the classifier\n",
    "#     classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "#     # predict the labels on validation dataset\n",
    "#     predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "#     if is_neural_net:\n",
    "#         predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "#     return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the dataset into training and validation datasets \n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(tweet_df_train['token_string'], tweet_df_train['gender'])\n",
    "\n",
    "# # label encode the target variable \n",
    "# encoder = preprocessing.LabelEncoder()\n",
    "# train_y = encoder.fit_transform(train_y)\n",
    "# valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "# # create a count vectorizer object \n",
    "# count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',  max_features=3000)\n",
    "# count_vect.fit(train_x)\n",
    "\n",
    "# # transform the training and validation data using count vectorizer object\n",
    "# xtrain_count =  count_vect.transform(train_x)\n",
    "# xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "# # word level tf-idf\n",
    "# tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}',  max_features=3000)\n",
    "# tfidf_vect.fit(tweet_df_train['token_string'])\n",
    "# xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "# xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# # ngram level tf-idf \n",
    "# tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3),  max_features=3000)\n",
    "# tfidf_vect_ngram.fit(tweet_df_train['token_string'])\n",
    "# xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "# xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes on Count Vectors\n",
    "# accuracy1 = train_model(naive_bayes.MultinomialNB(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "\n",
    "# # Naive Bayes on Word Level TF IDF Vectors\n",
    "# accuracy2 = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "\n",
    "# # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "# accuracy3 = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram.tocsc(), train_y, xvalid_tfidf_ngram.tocsc())\n",
    "\n",
    "# accuracy_token_df['NaiveBayes'] = [accuracy1, accuracy2, accuracy3]\n",
    "\n",
    "\n",
    "\n",
    "# # Linear Classifier on Count Vectors\n",
    "# accuracy5 = train_model(linear_model.LogisticRegression(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc().tocsc())\n",
    "\n",
    "# # Linear Classifier on Word Level TF IDF Vectors\n",
    "# accuracy6 = train_model(linear_model.LogisticRegression(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "\n",
    "# # Linear Classifier on Ngram Level TF IDF Vectors\n",
    "# accuracy7 = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram.tocsc(), train_y, xvalid_tfidf_ngram.tocsc())\n",
    "\n",
    "# accuracy_token_df.iloc[:,2] = [accuracy5, accuracy6, accuracy7]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # SVM on Ngram Level TF IDF Vectors\n",
    "# accuracy9 = train_model(svm.SVC(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "\n",
    "# # Linear Classifier on Word Level TF IDF Vectors\n",
    "# accuracy10 = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "\n",
    "# # Linear Classifier on Ngram Level TF IDF Vectors\n",
    "# accuracy11 = train_model(svm.SVC(), xtrain_tfidf_ngram.tocsc(), train_y, xvalid_tfidf_ngram.tocsc())\n",
    "\n",
    "# accuracy_token_df.iloc[:,3] = [accuracy9, accuracy10, accuracy11]\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn import decomposition, ensemble\n",
    "\n",
    "# # RF on Count Vectors\n",
    "# accuracy13 = train_model(ensemble.RandomForestClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "\n",
    "# # RF on Word Level TF IDF Vectors\n",
    "# accuracy14 = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "\n",
    "# # Linear Classifier on Ngram Level TF IDF Vectors\n",
    "# accuracy15 = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "# accuracy_token_df.iloc[:,4] = [accuracy13, accuracy14, accuracy15]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Extereme Gradient Boosting on Count Vectors\n",
    "# accuracy17 = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "\n",
    "# # Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "# accuracy18 = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "\n",
    "# accuracy19 = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "# accuracy_token_df.iloc[:, 5] = [accuracy17, accuracy18, accuracy19]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Multi-layer Perceptron on Count Vectors\n",
    "# accuracy21 = train_model(MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "\n",
    "# # Multi-layer Perceptron on Word Level TF IDF Vectors\n",
    "# accuracy22 = train_model(MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "\n",
    "# # # Multi-layer Perceptron on Character Level NGrams\n",
    "# accuracy23 = train_model(MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1), xtrain_tfidf_ngram.tocsc(), train_y, xvalid_tfidf_ngram.tocsc())\n",
    "# # print \"Multi-layer Perceptron, CharLevel Vectors: \", accuracy\n",
    "\n",
    "# accuracy_token_df.iloc[:, 6] = [accuracy21, accuracy22, accuracy23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's try the same with POS string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_pos_df = pd.DataFrame(np.nan, index=[0, 1, 2], columns=['Features', 'NaiveBayes', 'LogisticRegression', 'SupportVectorMachine', \n",
    "#                                                                 'RandomForest', 'XGBoost', 'NeuralNet'])\n",
    "# accuracy_pos_df['Features'] = ['CountVector', 'WordLevelTF_IDF', 'N-GramVector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the dataset into training and validation datasets \n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(tweet_df_train['pos_string'], tweet_df_train['gender'])\n",
    "\n",
    "# # label encode the target variable \n",
    "# encoder = preprocessing.LabelEncoder()\n",
    "# train_y = encoder.fit_transform(train_y)\n",
    "# valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "# # create a count vectorizer object \n",
    "# count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',  max_features=3000)\n",
    "# count_vect.fit(tweet_df_train['pos_string'])\n",
    "\n",
    "# # transform the training and validation data using count vectorizer object\n",
    "# xtrain_count =  count_vect.transform(train_x)\n",
    "# xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "# # word level tf-idf\n",
    "# tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}',  max_features=3000)\n",
    "# tfidf_vect.fit(tweet_df_train['pos_string'])\n",
    "# xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "# xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# # ngram level tf-idf \n",
    "# tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3),  max_features=3000)\n",
    "# tfidf_vect_ngram.fit(tweet_df_train['pos_string'])\n",
    "# xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "# xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes on Count Vectors\n",
    "# accuracy1 = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "# # Naive Bayes on Word Level TF IDF Vectors\n",
    "# accuracy2 = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "\n",
    "# # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "# accuracy3 = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "# accuracy_pos_df['NaiveBayes'] = [accuracy1, accuracy2, accuracy3]\n",
    "\n",
    "\n",
    "\n",
    "# # Linear Classifier on Count Vectors\n",
    "# accuracy5 = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "# # Linear Classifier on Word Level TF IDF Vectors\n",
    "# accuracy6 = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "\n",
    "# # Linear Classifier on Ngram Level TF IDF Vectors\n",
    "# accuracy7 = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "# accuracy_pos_df.iloc[:,2] = [accuracy5, accuracy6, accuracy7]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # SVM on Ngram Level TF IDF Vectors\n",
    "# accuracy9 = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "# # Linear Classifier on Word Level TF IDF Vectors\n",
    "# accuracy10 = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "\n",
    "# # Linear Classifier on Ngram Level TF IDF Vectors\n",
    "# accuracy11 = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "# accuracy_pos_df.iloc[:,3] = [accuracy9, accuracy10, accuracy11]\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn import decomposition, ensemble\n",
    "\n",
    "# # RF on Count Vectors\n",
    "# accuracy13 = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "# # RF on Word Level TF IDF Vectors\n",
    "# accuracy14 = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "\n",
    "# # Linear Classifier on Ngram Level TF IDF Vectors\n",
    "# accuracy15 = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "\n",
    "# accuracy_pos_df.iloc[:,4] = [accuracy13, accuracy14, accuracy15]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Extereme Gradient Boosting on Count Vectors\n",
    "# accuracy17 = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "\n",
    "# # Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "# accuracy18 = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "\n",
    "# accuracy19 = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "# accuracy_pos_df.iloc[:, 5] = [accuracy17, accuracy18, accuracy19]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Multi-layer Perceptron on Count Vectors\n",
    "# accuracy21 = train_model(MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "# # Multi-layer Perceptron on Word Level TF IDF Vectors\n",
    "# accuracy22 = train_model(MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "\n",
    "# # # Multi-layer Perceptron on Character Level NGrams\n",
    "# accuracy23 = train_model(MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "# # print \"Multi-layer Perceptron, CharLevel Vectors: \", accuracy\n",
    "\n",
    "# accuracy_pos_df.iloc[:, 6] = [accuracy21, accuracy22, accuracy23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's try this on the combination of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_comb_df = pd.DataFrame(np.nan, index=[0, 1, 2], columns=['Features', 'NaiveBayes', 'LogisticRegression', 'SupportVectorMachine', \n",
    "#                                                                 'RandomForest', 'XGBoost', 'NeuralNet'])\n",
    "# accuracy_comb_df['Features'] = ['CountVector', 'WordLevelTF_IDF', 'N-GramVector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the dataset into training and validation datasets \n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(tweet_df_train['pos_string'], tweet_df_train['gender'])\n",
    "\n",
    "# # label encode the target variable \n",
    "# encoder = preprocessing.LabelEncoder()\n",
    "# train_y = encoder.fit_transform(train_y)\n",
    "# valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "# # create a count vectorizer object \n",
    "# count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',  max_features=3000)\n",
    "# count_vect.fit(tweet_df_train['token_pos_string'])\n",
    "\n",
    "# # transform the training and validation data using count vectorizer object\n",
    "# xtrain_count =  count_vect.transform(train_x)\n",
    "# xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "# # word level tf-idf\n",
    "# tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}',  max_features=3000)\n",
    "# tfidf_vect.fit(tweet_df_train['token_pos_string'])\n",
    "# xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "# xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# # ngram level tf-idf \n",
    "# tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3),  max_features=3000)\n",
    "# tfidf_vect_ngram.fit(tweet_df_train['token_pos_string'])\n",
    "# xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "# xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes on Count Vectors\n",
    "# accuracy1 = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "# # Naive Bayes on Word Level TF IDF Vectors\n",
    "# accuracy2 = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "\n",
    "# # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "# accuracy3 = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "# accuracy_comb_df['NaiveBayes'] = [accuracy1, accuracy2, accuracy3]\n",
    "\n",
    "\n",
    "\n",
    "# # Linear Classifier on Count Vectors\n",
    "# accuracy5 = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "# # Linear Classifier on Word Level TF IDF Vectors\n",
    "# accuracy6 = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "\n",
    "# # Linear Classifier on Ngram Level TF IDF Vectors\n",
    "# accuracy7 = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "# accuracy_comb_df.iloc[:,2] = [accuracy5, accuracy6, accuracy7]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # SVM on Ngram Level TF IDF Vectors\n",
    "# accuracy9 = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "# # Linear Classifier on Word Level TF IDF Vectors\n",
    "# accuracy10 = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "\n",
    "# # Linear Classifier on Ngram Level TF IDF Vectors\n",
    "# accuracy11 = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "# accuracy_comb_df.iloc[:,3] = [accuracy9, accuracy10, accuracy11]\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn import decomposition, ensemble\n",
    "\n",
    "# # RF on Count Vectors\n",
    "# accuracy13 = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "# # RF on Word Level TF IDF Vectors\n",
    "# accuracy14 = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "\n",
    "# # RF on Ngram Level TF IDF Vectors\n",
    "# accuracy15 = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "\n",
    "\n",
    "# accuracy_comb_df.iloc[:,4] = [accuracy13, accuracy14, accuracy15]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Extereme Gradient Boosting on Count Vectors\n",
    "# accuracy17 = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "\n",
    "# # Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "# accuracy18 = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "\n",
    "# accuracy19 = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram.tocsc(), train_y, xvalid_tfidf_ngram.tocsc())\n",
    "\n",
    "# accuracy_comb_df.iloc[:, 5] = [accuracy17, accuracy18, accuracy19]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Multi-layer Perceptron on Count Vectors\n",
    "# accuracy21 = train_model(MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1), xtrain_count, train_y, xvalid_count)\n",
    "\n",
    "# # Multi-layer Perceptron on Word Level TF IDF Vectors\n",
    "# accuracy22 = train_model(MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "\n",
    "# # # Multi-layer Perceptron on Character Level NGrams\n",
    "# accuracy23 = train_model(MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1), xtrain_tfidf_ngram.tocsc(), train_y, xvalid_tfidf_ngram.tocsc())\n",
    "# # print \"Multi-layer Perceptron, CharLevel Vectors: \", accuracy\n",
    "\n",
    "# accuracy_comb_df.iloc[:, 6] = [accuracy21, accuracy22, accuracy23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our test label data and check the accuracy of out classifier predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_label_df = pd.read_csv(\"./data/test_labels.csv\")\n",
    "\n",
    "tweet_df_test = tweet_df_test.merge(testing_label_df[['id', 'gender']], on='id', how='left')\n",
    "tweet_df_test['gender'][tweet_df_test['gender'] == 'male'] = 1\n",
    "tweet_df_test['gender'][tweet_df_test['gender'] == 'female'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "The following commented out cell has pandas dataframes with the accuraies calculated for token string\n",
    "\n",
    "Now let's see how our accuracy dataframes are looking like\n",
    "\n",
    "The one with Token string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_token_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just the unigram tokens from Bag-Of-Words approach, the maximum accuracy we are getting is 76.2% with Logistic Regression and tf-idf vector. Let's use that and predict our test data and see the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(tweet_df_train['gender'])\n",
    "test_y = encoder.fit_transform(tweet_df_test['gender'])\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=3000)\n",
    "tfidf_vect.fit(tweet_df_train['token_string'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_tfidf =  tfidf_vect.transform(tweet_df_train['token_string'])\n",
    "xtest_tfidf =  tfidf_vect.transform(tweet_df_test['token_string'])\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(xtrain_tfidf, train_y)\n",
    "y_pred_XGB = classifier.predict(xtest_tfidf)\n",
    "accuracy_score(test_y, y_pred_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting an accuracy of 78.4%. \n",
    "\n",
    "The following commented out cell has pandas dataframes with the accuraies calculated for pos string\n",
    "\n",
    "Now let's see what POS String has got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_pos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest is for Logistic Regression withN-Gram vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=3000)\n",
    "tfidf_vect_ngram.fit(tweet_df_train['pos_string'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(tweet_df_train['pos_string'])\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(tweet_df_test['pos_string'])\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(xtrain_tfidf_ngram, train_y)\n",
    "y_pred_LR = classifier.predict(xtest_tfidf_ngram)\n",
    "accuracy_score(test_y, y_pred_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy we are getting here is 70%. \n",
    "\n",
    "The following commented out cell has pandas dataframes with the accuraies calculated for the combined token and pos string\n",
    "\n",
    "Hopefully for the combined BOW and POS String we'll get better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_comb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the combination model, we have NeuralNet with Count Vector as most accurate. Let's use this to predict our test data classification.\n",
    "\n",
    "\n",
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Linear regression and naive bayes count vectors\n",
    "# create a count vectorizer object \n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(tweet_df_train['gender'])\n",
    "test_y = encoder.fit_transform(tweet_df_test['gender'])\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features = 3000)\n",
    "count_vect.fit(tweet_df_train['token_pos_string'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(tweet_df_train['token_pos_string'])\n",
    "xtest_count =  count_vect.transform(tweet_df_test['token_pos_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1)\n",
    "clf.fit(xtrain_count, train_y)\n",
    "y_pred_MLP = clf.predict(xtest_count)\n",
    "accuracy_score(test_y, y_pred_MLP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Neural Network on Count vector of combined pos and bow tokens are giving us more accuracy. Let's write them to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.DataFrame(tweet_df_test['id'])\n",
    "test_csv['gender'] = y_pred_MLP\n",
    "test_csv['gender'][test_csv['gender'] == 1] = 'male'\n",
    "test_csv['gender'][test_csv['gender'] == 0] = 'female'\n",
    "test_csv.to_csv('pred_labels.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
